<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CS 180 Project 4</title>
    <link rel="stylesheet" type="text/css" href="/static/styling.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<header>
  <div id="header">
    <table id="TableHeader">
      <tr>
        <td>
            <span>
                <button onclick="location.href = '/index.html';" class="header"> Home </button>
            </span>
          </td>
        <td>
          <span>
            <button onclick="location.href = '/proj1/index.html';" class="header"> Project 1 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj2/index.html';" class="header"> Project 2 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj3/index.html';" class="header"> Project 3 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj4/index.html';" class="header"> Project 4 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj5/index.html';" class="header"> Project 5 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/projf/index.html';" class="header"> Final </button>
          </span>
        </td>
      </tr>
    </table>
</div>
</header>

<body>

<h1>Project 4</h1>

<div class="HomeContainer">
  <h2>Overview</h2>
  <h3 style="text-align: center;">In the first part of this project, we implemented a more generalized morphing/warping function that allows us to rectify images and to stitch together images into mosaics. In the second part, we automated the process by which the correspondences are made when stitching together mosaics. </h3>
</div>

<h1>Project 4A</h1>

<div class="HomeContainer">
  <h2>Taking Pictures</h2>
  <h3>Approach</h3>
  <p>Went around the Bay to take some picture.</p>

  <h3>Results</h3>
  <h4>Pictures for Rectification</h4>
  <table>
    <tr>
      <td> <img src="images/data/posters/andor.jpg" width="300" height="400"> </td>
      <td> <img src="images/data/posters/180blackboard.jpg" width="400" height="300"> </td>
    </tr>
  </table>

  <h4>SF Skyline</h4>
  <table>
    <tr>
      <td> <img src="images/data/sf/im1.jpg" width="300" height="400"> </td>
      <td> <img src="images/data/sf/im2.jpg" width="300" height="400"> </td>
    </tr>
  </table>

  <h4>Bay Area Sunset</h4>
  <table>
    <tr>
      <td> <img src="images/data/bay_sunset/im1.jpg" width="400" height="300"> </td>
      <td> <img src="images/data/bay_sunset/im2.jpg" width="400" height="300"> </td>
    </tr>
  </table>

  <h4>Bay Area Nighttime</h4>
  <table>
    <tr>
      <td> <img src="images/data/bay_night/im1.jpg" width="400" height="300"> </td>
      <td> <img src="images/data/bay_night/im2.jpg" width="400" height="300"> </td>
    </tr>
  </table>

  <h4>United B777 - Bay Fleet Week</h4>
  <table>
    <tr>
      <td> <img src="images/data/b777/im1.jpg" width="300" height="400"> </td>
      <td> <img src="images/data/b777/im2.jpg" width="300" height="400"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>Recovering Homographies</h2>
  <h3>Approach</h3>
  <p> The mathematics behind the image warping that is done in this project can be described with a linear transformation called a Homography or a perspective transform. The idea is that there exists a matrix which can project the pixels of an image onto the plane of perspective of another image. This allows us to see an image from a different perspective without having to retake the image. </p>
  
  <p> The Homography matrix is defined as:
    \[ \begin{bmatrix}
    a & b & c \\
    d & e & f \\
    g & h & 1
    \end{bmatrix}
    \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
    =
    \begin{bmatrix} wx' \\ wy' \\ w \end{bmatrix} \]
    Namely, is is first necessary to convert the pixel indices/coordiantes into the Barycentric coordiantes by extending them to the third dimension and appending a one as the third entry. Then, if we map a pixel through \(H\), we get the corresponding pixels's indices/coordinates in the projected plane, weighted by some constant. If we divide through by the third entry and keep only the first two, we obtain the projected coordinates. </p>
    
    <p> In this case, we don't know \(H\) but we do know the pixel values. This means that we can apply Least Squares to obtain the unknown coefficients in \(H\). This can be formulated as follows:
    \[ \begin{align*}
    &\begin{cases}
    ax + by + c = wx' \\
    dx + ey + f = wy' \\
    gx + hy + 1 = w
    \end{cases}
    \\
    \implies
    &\begin{cases}
    ax + by + c = (gx + hy + 1) x' \\
    dx + ey + f = (gx + hy + 1) y'
    \end{cases}
    \\
    \implies
    &\begin{cases}
    ax + by + c - gxx' - hyx' = x' \\
    dx + ey + f - gxy' - hyy' = y'
    \end{cases}
    \\
    \implies
    &\begin{bmatrix}
    x & y & 1 & 0 & 0 & 0 & -xx' & -yx' \\
    0 & 0 & 0 & x & y & 1 & -xy' & -yy' \\
    \end{bmatrix}
    \begin{bmatrix} a \\ b \\ c \\ d \\ e \\ f \\ g \\ h \end{bmatrix}
    =
    \begin{bmatrix} x' \\ y' \end{bmatrix}
    \end{align*} \] 
    If we repeat this process for three other pixels, we will have the necessary eight equations to solve for the eight unknowns of \(H\). Thus, we need to <b>choose</b> which pixels should define our Homography. This can be done by choosing specific features on the image that will be warped/projected and matching them to desired projected pixels. This part also uses this <a href="https://cal-cs180.github.io/fa23/hw/proj3/tool.html">very useful website</a> again to choose these correspondence points. 
  </p>
  
</div>

<div class="HomeContainer">
  <h2>Warping Images</h2>
  <h3>Approach</h3>
  <p>Once we have computed \(H\) for an image, we need to warp/project all of the pixels of that image onto the desired plane. We do this by following a very similar procedure to the one done in the <a href="/proj3/index.html">previous project</a>. However, it is not necessary to perform any triangulation for this project. Here is an overview of the procedure that I implemented:</p>
  <ol>
    <li> Map the corners of the original image through \(H\). These new pixels are the corners of the warped image. </li>
    <li> These projected corners might have negative values, so I create a second copy of the warped corners and align them such that the smallest entry in this list is zero. These are the shifted corners. </li>
    <li> Find the shape of the warped image using the warped corners: max(warped corners) - min(warped corners). Then, place the shifted corners into the skimage.draw.polygon function to get the region that our image will be projected onto. </li>
    <li> Remove the shift introduced in the region and perform inverse warping by mapping the entire region calculated above through \(H^{-1}\). It's necessary to remove the shift, otherwise, we don't map back to the original image correctly. </li>
    <li> Now that we have the pixels we want to sample at in the original image, perform interpolation using scipy.interpolate.griddata to get the value at those pixels for each color channel. </li>
    <li> Put everything together and place the warped image into an empty array by indexing at the polygon region. </li>
  </ol>
  <p>The key in all of this is that you have to use the un-shifted version of the polygon region to obtain the corresponding sampling points in the original image. But use the shifted version of the polygon region to map into the new warped image.</p>
</div>

<div class="HomeContainer">
  <h2>Rectified Images</h2>
  <h3>Approach</h3>
  <p>Using the selected correspondences, I warp the image into a rectangle in the middle of the projected image. I compute the Homography \(H\) using these correspondences and compute the warp as described above. This procedure yielded the following results. </p>

  <h3>Results</h3>
  <table>
    <tr>
      <th>Original Image</th>
      <th>Rectified</th>
    </tr>
    <tr>
      <td> <img src="images/data/posters/andor.jpg" width="300" height="400"> </td>
      <td> <img src="images/processed/posters/andor.jpg" width="400" height="400"> </td>
    </tr>
    <tr>
      <td> <img src="images/data/posters/180blackboard.jpg" width="400" height="300"> </td>
      <td> <img src="images/processed/posters/180blackboard.jpg" width="400" height="400"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>Mosaics</h2>
  <h3>Approach</h3>
  <p> In this part, we take two images. Keep one fixed and project the other onto the perspective plane of that image. Put the two images and blend them together to create a mosaic. Below is the procedure that I implemented: </p>
  <ol>
    <li> Project an image onto a fixed image using the procedures described above. </li>
    <li> Find the size of the image buffer that will hold the two blended images.</li>
    <li> Make two copies of an empty buffer with that size and place the warped and the fixed image in each. Also, add an alpha channel for the images: wherever a pixel in the image exists, set alpha to 1, and 0 otherwise. </li>
    <li> Using the alpha channel, compute the shortest distance to an edge for all the pixels in both images using scipy.ndimage.distance_transform_edt. Then, if we compare the two arrays of distances, we can obtain a boolean array that can act as a mask for the next step. </li>
    <li> To seamlessly blend the two images together, compute a Laplacian stack with 2 levels using the implementation from <a href="/proj2/index.html">project 2</a>. </li>
  </ol>

  <h3>Results</h3>
  <p> Note that for the picture of the Boeing 777, I changed the center of projection when I took the pictures. This means that the alignment between the two pictures is severely affected. </p>
  <table>
    <tr>
      <th>San Francisco Skyline</th>
    </tr>
    <tr>
      <td> <img src="images/processed/sf/mosaic.jpg" width="600" height="600"> </td>
    </tr>
    <tr>
      <th>The Bay Sunset</th>
    </tr>
    <tr>
      <td> <img src="images/processed/bay_sunset/mosaic.jpg" width="600" height="600"> </td>
    </tr>
    <tr>
      <th>The Bay Nighttime</th>
    </tr>
    <tr>
      <td> <img src="images/processed/bay_night/mosaic.jpg" width="600" height="600"> </td>
    </tr>
    <tr>
      <th>Fleet Week (not the best)</th>
    </tr>
    <tr>
      <td> <img src="images/processed/b777/mosaic.jpg" width="600" height="600"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>Bells and Whistles</h2>
  None for this project.
</div>

<h1>Project 4B</h1>

<div class="HomeContainer">
  <h2>Harris Corners and Adaptive Non-Maximal Suppression (ANMS)</h2>
  <h3>Approach</h3>
  <p>In this part, we want to find points of interest which we can use as features to automatically build the correspondences between images in a mosaic. These points of interest are called Harris corners. Using the provided starter code, the Harris corners along with their scores are calculated. Then, the ANMS algorithm allows us to suppress the vast majority of the Harris corners while maintaining a good distribution of them across the image. This is where we use the equation in the paper which introduced ANMS. The paper is titled “Multi-Image Matching using Multi-Scale Oriented Patches". 
  \[ r_i = \min_j |x_i - x_j| \,\,\, \text{ s.t. } \,\,\, h(x_i) &lt; c_{\text{robust}} h(x_j) \]
  Compute this for all corners \(x_i\). Then, return a desired number of best corners (I chose to return the best 300 and I also set \(c_{\text{robust}} = 0.9\)), where a corner is best if it didn't get suppressed by other corners. This ensures that the Harris corners don't clump up and are distributed evenly across the picture.

  </p>

  <h3>Results</h3>
  <p>The images on the left include all of the Harris corners while the images on the right include the corners after ANMS. </p>

  <h4>SF Skyline</h4>
  <table>
    <tr>
      <td> <img src="images/processed/sf/corners_im1.jpg" width="500" height="500"> </td>
      <td> <img src="images/processed/sf/corners_im2.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>Bay Area Sunset</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_sunset/corners_im1.jpg" width="500" height="500"> </td>
      <td> <img src="images/processed/bay_sunset/corners_im2.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>Bay Area Nighttime</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_night/corners_im1.jpg" width="500" height="500"> </td>
      <td> <img src="images/processed/bay_night/corners_im2.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>United B777 - Bay Fleet Week</h4>
  <table>
    <tr>
      <td> <img src="images/processed/b777/corners_im1.jpg" width="500" height="500"> </td>
      <td> <img src="images/processed/b777/corners_im2.jpg" width="500" height="500"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>Feature Descriptor Extraction</h2>
  <h3>Approach</h3>
  <p> Now that we have the points of interest, we want to extract the feature that makes this point interesting. By doing this, we can reference the same features across images even if the exact pixel values are not the same. This is done by taking a 40 by 40 window from the original image centered at each ANMS corner and resizing this window to be 8 by 8. Finally, demean and divide by the standard deviation to normalize the feature. This blurs the pixel values and allows us to compare features across images. </p>
  
  <h3>Results</h3>
  <p> Some of the features extracted as mentioned above. Note that these are not the normalized features. </p>
  
  <h4>SF Skyline</h4>
  <table>
    <tr>
      <td> <img src="images/processed/sf/patches/patch_0.jpg" width="300" height="400"> </td>
      <td> <img src="images/processed/sf/patches/patch_1.jpg" width="300" height="400"> </td>
    </tr>
  </table>

  <h4>Bay Area Sunset</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_sunset/patches/patch_0.jpg" width="400" height="300"> </td>
      <td> <img src="images/processed/bay_sunset/patches/patch_1.jpg" width="400" height="300"> </td>
    </tr>
  </table>

  <h4>Bay Area Nighttime</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_night/patches/patch_0.jpg" width="400" height="300"> </td>
      <td> <img src="images/processed/bay_night/patches/patch_1.jpg" width="400" height="300"> </td>
    </tr>
  </table>

  <h4>United B777 - Bay Fleet Week</h4>
  <table>
    <tr>
      <td> <img src="images/processed/b777/patches/patch_0.jpg" width="300" height="400"> </td>
      <td> <img src="images/processed/b777/patches/patch_1.jpg" width="300" height="400"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>Feature Matching</h2>
  <h3>Approach</h3>
  <p> Now that we have the features for the images of the mosaic and the list of the corners from ANMS, we want to put everything together to obtain the matches for the corners. This is performed with Lowe's trick.
  <ol>
    <li>Take the pairwise difference of all of the feature patches from the previous part. Compute the norm squared of these differences to obtain a single value for each pair of features. </li>
    <li>Make a copy of these values, sort them, and call these sorted values the nearest neighbors. </li>
    <li>Apply Lowe's trick by taking the ratio of of the 1st nearest neighbor and the 2nd nearest neighbor. If this ratio is smaller than a certain threshold (0.5), then accept a feature's match with its 1st closest neighbor. Otherwise reject it. </li>
    <li>Return the set of matched corners. </li>
  </ol>  
  </p>

  <h3>Results</h3>
  <p>For each of the images below, you can see the matched corners after ANMS for both images. </p>

  <h4>SF Skyline</h4>
  <table>
    <tr>
      <td> <img src="images/processed/sf/anms_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>Bay Area Sunset</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_sunset/anms_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>Bay Area Nighttime</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_night/anms_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>United B777 - Bay Fleet Week</h4>
  <table>
    <tr>
      <td> <img src="images/processed/b777/anms_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>RANSAC</h2>
  <h3>Approach</h3>
  <p>Now that we have matched corners, we want to select the combination of 4 corners which will yield the most accurate homography. 
    <ol>
      <li>Choose 4 pairs of corners without replacement at random.</li>
      <li>Compute the homography and map all of the source corners to the target corners through the homography.</li>
      <li>Compare how well these points get mapped by thresholding on the euclidean norm.</li>
      <li>Count how many mapped points are close enough to the original target points.</li>
      <li>Take the max over how many points are “inline" and return the subset of corners that maximizes this value. </li>
    </ol>
    This subset of points are the correspondences that will be used to warp images. As a final note, I'm unsure of the exact reason why but RANSAC was a bit “finnicky" with the sunset and nighttime pictures. There seems to be one combination of points that achieves the max inline correspondences but whose homography scales the points way too much. Thus, I always reseeded at the beginning of each call to RANSAC. </p>

  <h3>Results</h3>
  <h4>SF Skyline</h4>
  <table>
    <tr>
      <td> <img src="images/processed/sf/ransac_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>Bay Area Sunset</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_sunset/ransac_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>Bay Area Nighttime</h4>
  <table>
    <tr>
      <td> <img src="images/processed/bay_night/ransac_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>

  <h4>United B777 - Bay Fleet Week</h4>
  <table>
    <tr>
      <td> <img src="images/processed/b777/ransac_matched_corners.jpg" width="500" height="500"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>Mosaics</h2>
  <h3>Approach</h3>
  <p> With these correspondence points, compute the mosaic as was done in Project 4A. </p>

  <h3>Results</h3>
  <p>Comparing the manual (left) and auto (right) correspondence points. Note, the B777 mosaic is much better now, compared to when I manually picked the correspondence points. </p>

  <table>
    <tr>
      <th>San Francisco Skyline - Manual</th>
      <th>San Francisco Skyline - Auto</th>
    </tr>
    <tr>
      <td> <img src="images/processed/sf/mosaic.jpg" width="300" height="400"> </td>
      <td> <img src="images/processed/sf/mosaic_auto.jpg" width="300" height="400"> </td>
    </tr>
    <tr>
      <th>The Bay Sunset - Manual</th>
      <th>The Bay Sunset - Auto</th>
    </tr>
    <tr>
      <td> <img src="images/processed/bay_sunset/mosaic.jpg" width="400" height="300"> </td>
      <td> <img src="images/processed/bay_sunset/mosaic_auto.jpg" width="400" height="300"> </td>
    </tr>
    <tr>
      <th>The Bay Nighttime - Manual</th>
      <th>The Bay Nighttime - Auto</th>
    </tr>
    <tr>
      <td> <img src="images/processed/bay_night/mosaic.jpg" width="400" height="300"> </td>
      <td> <img src="images/processed/bay_night/mosaic_auto.jpg" width="400" height="300"> </td>
    </tr>
    <tr>
      <th>Fleet Week - Manual</th>
      <th>Fleet Week - Auto</th>
    </tr>
    <tr>
      <td> <img src="images/processed/b777/mosaic.jpg" width="300" height="400"> </td>
      <td> <img src="images/processed/b777/mosaic_auto.jpg" width="300" height="400"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>What I have learned.</h2>
  <p>I think that the entire formulation on how to find good features and match correspondences is an interesting solution to a difficult problem. I think that the feature matching is one of the more interesting parts of the project; especially the use of Lowe's trick to solve the problem. It is also cool to see that the B777 picture now lines up pretty well even if I didn't employ the best picture taking techniques. </p>
</div>

<div class="HomeContainer">
  <h2>Bells and Whistles</h2>
  None for this project.
</div>

</body>
</html>