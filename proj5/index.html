<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CS 180 Project 5</title>
    <link rel="stylesheet" type="text/css" href="/static/styling.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<header>
  <div id="header">
    <table id="TableHeader">
      <tr>
        <td>
            <span>
                <button onclick="location.href = '/index.html';" class="header"> Home </button>
            </span>
          </td>
        <td>
          <span>
            <button onclick="location.href = '/proj1/index.html';" class="header"> Project 1 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj2/index.html';" class="header"> Project 2 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj3/index.html';" class="header"> Project 3 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj4/index.html';" class="header"> Project 4 </button>
          </span>
        </td>
        <td>
          <span>
            <button onclick="location.href = '/proj5/index.html';" class="header"> Project 5 </button>
          </span>
        </td>
      </tr>
    </table>
</div>
</header>

<body>

<h1>Project 5</h1>
<div class="HomeContainer">
  <h2>Overview</h2>
  <h3 style="text-align: center;"> This project was all about Diffusion and the neural network implementation, UNets, which allow us to denoise an image of random noise into an actual image. </h3>
</div>

<h1>Project 5A</h1>
<div class="HomeContainer">
  <h2>Overview</h2>
  <h3 style="text-align: center;"> The first part of the project uses the DeepFloyd model's parameters and implementation to make our own denoisers and also do different things with these denoisers. The focus here is to rebuild the sampling model using the already trained model. Throughout this project, I always reseed with seed=1729 before running any sampling/random process. </h3>
</div>

<div class="HomeContainer">
  <h2>1.0: Using DeepFloyd</h2>
  <h3>Approach</h3>
  <p>For this first section, we only had to make a prompt and obtain its text embedding, which the DeepFloyd model also provides, and sample the model for the following images. The prompts used are: “an oil painting of a snowy mountain village", “a man wearing a hat", and “a rocket ship". The model has two parts: the first one samples the image and the second one upsamples the output of the first part since this output is only 64 x 64 pixels. Furthermore, by increasing the number of steps that the model can perform in the sampling and upsampling process, the better/more detailed images it can generate. This can be seen in the rocket images where the number of inference steps is 50.</p>

  <h3>Results</h3>
  <p>I made both images appear to have the same size but notice that the pictures on the left are more pixelated; these are the ones that were not upsampled. </p>

  <h4>an oil painting of a snowy mountain village</h4>
  <table>
    <tr>
      <td> <img src="images_5a/Results/Part 0/village_64_20.png" width="400" height="400"> </td>
      <td> <img src="images_5a/Results/Part 0/village_256_20.png" width="400" height="400"> </td>
    </tr>
  </table>

  <h4>a man wearing a hat</h4>
  <table>
    <tr>
      <td> <img src="images_5a/Results/Part 0/man_64_20.png" width="400" height="400"> </td>
      <td> <img src="images_5a/Results/Part 0/man_256_20.png" width="400" height="400"> </td>
    </tr>
  </table>

  <h4>a rocket ship (20)</h4>
  <table>
    <tr>
      <td> <img src="images_5a/Results/Part 0/rocket_64_20.png" width="400" height="400"> </td>
      <td> <img src="images_5a/Results/Part 0/rocket_256_20.png" width="400" height="400"> </td>
    </tr>
  </table>

  <h4>a rocket ship (50)</h4>
  <table>
    <tr>
      <td> <img src="images_5a/Results/Part 0/rocket_64_50.png" width="400" height="400"> </td>
      <td> <img src="images_5a/Results/Part 0/rocket_256_50.png" width="400" height="400"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.1: Forward Process</h2>
  <h3>Approach</h3>
  <p>For this section, we implement the noising function which adds a certain amount of noise to an image. The scaling is determined by already trained parameters in the DeepFloyd model. The formula for this is: \[ x_t = \sqrt{\overline{\alpha}_t} \, x_0 + \sqrt{1- \overline{\alpha}_t} \, \epsilon \qquad \epsilon \sim \mathcal{N}(0,1) \]
    where \(t\) can go from 1 to 1000, and a higher \(t\) corresponds to a noiser image. Using the provided test image, add noise for \(t = 250, 500, 750\).
  </p>

  <h3>Results</h3>
  <table>
    <tr>
      <td> t=0 </td>
      <td> t=250 </td>
      <td> t=500 </td>
      <td> t=750 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 1/t0.png" width="200" height="200"> </td>
      <td> <img src="images_5a/Results/Part 1/t250.png" width="200" height="200"> </td>
      <td> <img src="images_5a/Results/Part 1/t500.png" width="200" height="200"> </td>
      <td> <img src="images_5a/Results/Part 1/t750.png" width="200" height="200"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.2: Classical Denoise</h2>
  <h3>Approach</h3>
  <p>Usually, we would denoise a picture by applying a Gaussian blur filter (low pass filter with kernel size 7 and sigma 2) to the image. We do this because noise tends to be high frequency, so it gets filtered out whenever we apply this blur. Below are the results.
  </p>

  <h3>Results</h3>
  <table>
    <tr>
      <td> t=250 </td>
      <td> t=500 </td>
      <td> t=750 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 2/t250.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 2/t500.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 2/t750.png" width="300" height="300"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.3: One-Step Denoise</h2>
  <h3>Approach</h3>
  <p>The denoising done previously is not very good, so let's use the Diffusion model. The Diffusion model can predict the amount of noise in the image so that it can be removed. This is where the UNet, the main neural network component/architecture, of the model resides. It was trained so that it can predict the noise in an image given the noisy image and the current time step \(t\). This uses the same formula as 1.1, but we solve for \(x_0\), so we have \[ \hat{x}_0 = \frac{1}{\sqrt{\overline{\alpha}_t}} (x_t - \sqrt{1- \overline{\alpha}_t} \, \hat{\epsilon}) \] where \(\hat{\epsilon}\) is the predicted noise in the image. Notice that this will be an approximate image since the model is making a prediction. This can be seen below where the original noisier image looks very different to the original. However, the model removed the noise a lot better than Gaussian blurring did. </p>

  <h3>Results</h3>
  <table>
    <tr>
      <td> t=250 </td>
      <td> t=500 </td>
      <td> t=750 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 3/t250.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 3/t500.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 3/t750.png" width="300" height="300"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.4: Iterative Denoise</h2>
  <h3>Approach</h3>
  <p> Diffusion models are designed to iteratively denoise an image, step-by-step, instead of just taking one huge leap. We implement that part of the model here using the following formula:
    \[x_{t'} = \frac{\sqrt{\overline{\alpha}_{t'}} \, \beta_t}{1-\overline{\alpha}_{t}}\hat{x}_0 + \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t'})}{1-\overline{\alpha}_{t}}x_t + v_\sigma  \]
    where \(t > t', \,\, \alpha_t = \overline{\alpha}_t/\overline{\alpha}_{t'}, \,\, \beta_t = 1-\alpha_t\), \(\hat{x}_0\) is the current prediction for the original image using \(x_t\), so the equation written in section 1.3, and \(v_\sigma\) is the predicted variance in the noise. Recall that we want to go from larger to smaller \(t\). </p>
    <p> These images were made by firstly noising the original image to time 690 and then iteratively denoising as explained above. Note that it is possible to skip values of \(t\), so our schedule decreases \(t\) not by 1 but by 30 at each iteration.  </p>

  <h3>Results</h3>
  <h4>Some samples of the iterative denoising process</h4>
  <table>
    <tr>
      <td> t=690 </td>
      <td> t=540 </td>
      <td> t=390 </td>
      <td> t=240 </td>
      <td> t=90 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 4/t690.png" width="200" height="200"> </td>
      <td> <img src="images_5a/Results/Part 4/t540.png" width="200" height="200"> </td>
      <td> <img src="images_5a/Results/Part 4/t390.png" width="200" height="200"> </td>
      <td> <img src="images_5a/Results/Part 4/t240.png" width="200" height="200"> </td>
      <td> <img src="images_5a/Results/Part 4/t90.png" width="200" height="200"> </td>
    </tr>
  </table>

  <h4>Comparing the three different denoising methods</h4>
  <table>
    <tr>
      <td> Blurring </td>
      <td> One-Step </td>
      <td> Iterative </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 4/blurring.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 4/one_step.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 4/iterative.png" width="300" height="300"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.5: Diffusion Sampling</h2>
  <h3>Approach</h3>
  <p>Using the above procedure, generate an image of purely random Gaussian noise and feed it into the model starting with time 990. The Diffusion model will denoise this into some image. </p>

  <h3>Results</h3>
  <table>
    <tr>
      <td> Sample 1 </td>
      <td> Sample 2 </td>
      <td> Sample 3 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 5/sample1.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 5/sample2.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 5/sample3.png" width="300" height="300"> </td>
    </tr>
  </table>
  <table>
    <tr>
      <td> Sample 4 </td>
      <td> Sample 5 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 5/sample4.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 5/sample5.png" width="300" height="300"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.6: Classifier Free Guidance (CFG) </h2>
  <h3>Approach</h3>
  <p> Up to now, all of the calls to the Diffusion model have had a prompt that was acting as the null/empty prompt. This prompt is “a high quality photo". However, we want the model to be able to compare the noise of a true null prompt or completely unconditioned denoising to the prompt that we actually want to work towards. This is achieved by taking into account the unconditional noise which is defined as the noise for the null prompt/class, which is literally the empty string. This gives us a new noise estimation: \[ \epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u) \] where \(\gamma\) is a scaling factor which essentially controls the strength of the CFG. By setting \(\gamma >1\), we can actually get much more higher quality images. In terms of the actual implementation, everything else is exactly the same as it was before. </p>

  <h3>Results</h3>
  <p>Note that these images were upsampled using a different method compared to the non-CFG, so that's why there's a notable difference in quality. Despite this, compared to the samples without CFG, we get much better quality in these images. </p>
  <table>
    <tr>
      <td> Sample 1 </td>
      <td> Sample 2 </td>
      <td> Sample 3 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 6/sample4.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 6/sample5.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 6/sample8.png" width="300" height="300"> </td>
    </tr>
  </table>
  <table>
    <tr>
      <td> Sample 4 </td>
      <td> Sample 5 </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 6/sample9.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 6/sample10.png" width="300" height="300"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.7: Image to Image Translation </h2>
  <h3>Approach</h3>
  <p> For this part, simply noise a test image up to some value of \(t\) determined by i_start (lower i_start is higher \(t\)) and then denoise with CFG. </p>

  <h3>Results</h3>
  <h4>Campanille SDEdit (image-2-image translation) for different i_start </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7/campanille_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/campanille_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/campanille_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/campanille_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/campanille_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/campanille_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.2/campanille/original.png" width="150" height="150"> </td>
    </tr>
  </table>
  
  <h4>Beach SDEdit (image-2-image translation) for different i_start </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7/beach_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/beach_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/beach_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/beach_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/beach_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/beach_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Data/beach.jpg" width="150" height="150"> </td>
    </tr>
  </table>

  <h4>Dog SDEdit (image-2-image translation) for different i_start </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7/dog_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/dog_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/dog_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/dog_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/dog_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7/dog_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Data/dog.jpg" width="150" height="150"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.7.1: Image to Image Translation for a Web image and Hand-Drawn images </h2>
  <h3>Approach</h3>
  <p> Exact approach as 1.7 but with more images. These images were not upsampled. </p>

  <h3>Results</h3>
  <h4>Chill Guy (web image) for different i_start </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.1/chillguy_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/chillguy_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/chillguy_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/chillguy_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/chillguy_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/chillguy_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/chillguy_edit/chillguy.png" width="150" height="150"> </td>
    </tr>
  </table>
  
  <h4>F22 (drawn image) for different i_start </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.1/f22_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/f22_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/f22_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/f22_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/f22_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/f22_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/f22_edit/f22.png" width="150" height="150"> </td>
    </tr>
  </table>

  <h4>Troll Face (drawn image) for different i_start </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.1/troll_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/troll_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/troll_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/troll_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/troll_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/troll_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.1/troll_edit/troll.png" width="150" height="150"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.7.2: Image Inpainting </h2>
  <h3>Approach</h3>
  <p> Replace a selected region of the image with new content generated by the Diffusion Model with CFG. Before each iteration, update the current \(x_t\) as follows: \[ x_t \leftarrow m \cdot x_t + (1 - m) \cdot \text{forward}(x_0, t) \] where \(m\) is a mask with the same dimensions as the image and has 1s wherever we want to place the new content and 0s elsewhere.  </p>

  <h3>Results</h3>
  <h4> Campanille </h4>
  <table>
    <tr>
      <td> Original </td>
      <td> Mask </td>
      <td> Selected Region </td>
      <td> Result </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.2/campanille/original.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/campanille/mask.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/campanille/masked_region.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/campanille/result.png" width="250" height="250"> </td>
    </tr>
  </table>
  
  <h4> Dog 2 </h4>
  <table>
    <tr>
      <td> Original </td>
      <td> Mask </td>
      <td> Selected Region </td>
      <td> Result </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.2/dog/original.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/dog/mask.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/dog/masked_region.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/dog/result.png" width="250" height="250"> </td>
    </tr>
  </table>

  <h4> Bridge </h4>
  <table>
    <tr>
      <td> Original </td>
      <td> Mask </td>
      <td> Selected Region </td>
      <td> Result </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.2/bridge/original.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/bridge/mask.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/bridge/masked_region.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/bridge/result.png" width="250" height="250"> </td>
    </tr>
  </table>

  <h4> Flower </h4>
  <table>
    <tr>
      <td> Original </td>
      <td> Mask </td>
      <td> Selected Region </td>
      <td> Result </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.2/flower/original.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/flower/mask.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/flower/masked_region.png" width="250" height="250"> </td>
      <td> <img src="images_5a/Results/Part 7.2/flower/result.png" width="250" height="250"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.7.3: Image to Image Translation with Specific Prompt </h2>
  <h3>Approach</h3>
  <p> Exact approach as 1.7 and 1.7.1 but now we specify a prompt instead of just passing in the generic “a high quality photo" prompt. Essentially, by providing a prompt, we are “guiding" the denoising process. </p>

  <h3>Results</h3>
  <h4>Campanille - “a rocket ship" </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.3/campanille_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/campanille_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/campanille_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/campanille_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/campanille_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/campanille_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.2/campanille/original.png" width="150" height="150"> </td>
    </tr>
  </table>
  
  <h4>Beach - “a photo of the amalfi coast" </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.3/beach_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/beach_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/beach_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/beach_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/beach_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/beach_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Data/beach.jpg" width="150" height="150"> </td>
    </tr>
  </table>

  <h4>Dog - “a photo of a dog" </h4>
  <table>
    <tr>
      <td> i_start = 1 </td>
      <td> i_start = 3 </td>
      <td> i_start = 5 </td>
      <td> i_start = 7 </td>
      <td> i_start = 10 </td>
      <td> i_start = 20 </td>
      <td> Original </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 7.3/dog_edit/istart1.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/dog_edit/istart3.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/dog_edit/istart5.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/dog_edit/istart7.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/dog_edit/istart10.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Results/Part 7.3/dog_edit/istart20.png" width="150" height="150"> </td>
      <td> <img src="images_5a/Data/dog.jpg" width="150" height="150"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.8: Visual Anagrams</h2>
  <h3>Approach</h3>
  <p> The main idea behind anagrams is that we want a picture to contain two pictures. One of them is visible in one direction and flipping the image the other way around causes the second one to become visible. This can be done by “blending" together the predicted noise during the sampling process for two different prompts. Mathematically, the new noise estimate is calculated by: \[ \epsilon_1 = \text{UNet}(x_t, t, p_1), \qquad \epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2)), \qquad \epsilon = (\epsilon_1 + \epsilon_2) / 2 \] where flip(\(\cdot\)) flips an image and \(p_1,p_2\) are the two different prompts. Everything else is done the exact same way as in CFG. </p>

  <h3>Results</h3>
  <p> I had to sample many times using different seeds until I got decent enough results. (+0, +6, +0) respectively added to the base seed=1729 </p>
  <table>
    <tr>
      <td> an oil painting of an old man </td>
      <td> an oil painting of people around a campfire </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 8/man_campfire.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 8/man_campfire_reverse.png" width="300" height="300"> </td>
    </tr>
  </table>
  <table>
    <tr>
      <td> a painting of abraham lincoln's face </td>
      <td> a painting of a lion </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 8/abraham_lion2.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 8/abraham_lion2_reverse.png" width="300" height="300"> </td>
    </tr>
  </table>
  <table>
    <tr>
      <td> a golden eagle </td>
      <td> a fighter jet </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 8/eagle_jet.png" width="300" height="300"> </td>
      <td> <img src="images_5a/Results/Part 8/eagle_jet_reverse.png" width="300" height="300"> </td>
    </tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>1.9: Hybrid Images</h2>
  <h3>Approach</h3>
  <p> The main idea behind hybrid images is that we want a picture to contain two pictures again. One of them is visible from far away (contains the low frequencies of the image) and the other is visible up close (contains the high frequencies of the image). This can be done by “blending" together the predicted noise during the sampling process for two different prompts. Mathematically, the new noise estimate is calculated by: \[ \epsilon_1 = \text{UNet}(x_t, t, p_1), \qquad \epsilon_2 = \text{UNet}(x_t, t, p_2), \qquad \epsilon = f_{\text{LP}}(\epsilon_1) + f_{\text{HP}}(\epsilon_2) \] where \(f_{\text{LP}}, f_{\text{HP}}\) are a low pass and high pass filters, respectively. These were implemented with Gaussian blurring using kernel of size 33 and sigma 2 for the low pass and kernel size 11 and sigma 2 for the high pass (image - low pass is the high frequencies). Everything else is done the exact same way as in CFG. </p>

  <h3>Results</h3>
  <p> Similarly to 1.8, I had to sample many times using different seeds until I got decent enough results. (+0, +19, +4) respectively added to the base seed=1729 </p>
  <table>
    <tr>
      <td> a lithograph of a skull </td>
      <td> a lithograph of waterfalls </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 9/skull_waterfall.png" width="50" height="50"> </td>
      <td> <img src="images_5a/Results/Part 9/skull_waterfall.png" width="300" height="300"> </td>
    </tr>
  </table>
  <table>
    <tr>
      <td> a polar bear </td>
      <td> a dense snowy forest </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 9/tree_bear.png" width="50" height="50"> </td>
      <td> <img src="images_5a/Results/Part 9/tree_bear.png" width="300" height="300"> </td>
    </tr>
  </table>
  <table>
    <tr>
      <td> a painting of a green bird </td>
      <td> a painting of tree leaves </td>
    </tr>
    <tr>
      <td> <img src="images_5a/Results/Part 9/leaf_bird.png" width="50" height="50"> </td>
      <td> <img src="images_5a/Results/Part 9/leaf_bird.png" width="300" height="300"> </td>
    </tr>
  </table>
</div>

<h1>Project 5B</h1>
<div class="HomeContainer">
  <h2>Overview</h2>
  <h3 style="text-align: center;"> The second part of the project is implementing the UNet architecture, including time and class conditioning, and training the neural net on the MNIST dataset to essentially generate new images that would belong to the MNIST dataset. </h3>
</div>

<div class="HomeContainer">
  <h2>2.1.1 and 2.1.2: Training and Sampling the Unconditioned UNet</h2>
  <h3>Approach</h3>
  <img class="nonboxed" src="images_5b/Part1/unconditional_arch.png" width="800" height="400"> 
  <p> This image is taken directly from the CS 180 website specifications. For this first part of the project, I implemented this neural network architecture using PyTorch nn modules. The neural net minimizes the MSE Loss: \[ \mathbb{E}_{x,z} \| D_\theta(z) - x \|^2 \] where \(x\) is the image in the MNIST dataset, \(z = x + \sigma\epsilon \) is the noised image where \(\epsilon \sim \mathcal{N}(0,I) \) and \( \sigma \in \{ 0.0,0.2,0.4,0.5,0.6,0.8,1.0 \} \). The model \(D_\theta(\cdot)\) then tries to recover the original image from the given noised image. For training, \(\sigma = 0.5\) fixed, the hidden layer has dimension 128, and use Adams optimizer with \(10^{-4}\) learning rate. </p>

  <h3>Results</h3>
  <h4>Figure 3: Images in the MNIST dataset noised at different levels, according to \(\sigma\) above.</h4>
  <img class="nonboxed" src="images_5b/Part1/figure3.png" width="800" height="400"> 

  <h4>Figure 4: Training Loss</h4>
  <img class="nonboxed" src="images_5b/Part1/figure4.png" width="600" height="500"> 

  <table>
    <tr>
      <td>Figure 5: Model Output after 1st Epoch on Test images</td>
      <td>Figure 6: Model Output after 5th Epoch on Test images</td>
    </tr>
    <tr>
      <td> <img src="images_5b/Part1/figure5.png" width="300" height="250"> </td>
      <td> <img src="images_5b/Part1/figure6.png" width="300" height="250">  </td>
    </tr>
  </table>
  
  <h4>Figure 7: Model Output - out of distribution</h4>
  <p>This means that the input to the model is an image with \(\sigma\) different to 0.5. </p>
  <img class="nonboxed" src="images_5b/Part1/figure7.png" width="800" height="400"> 
  
</div>

<div class="HomeContainer">
  <h2>2.2.1 - 2.2.3: Training and Sampling the Time-Conditioned Diffusion Model</h2>
  <h3>Approach</h3>
  <img class="nonboxed" src="images_5b/Part2/conditional_arch.png" width="800" height="400"> 
  <p> This image is taken directly from the CS 180 website specifications. For this part of the project, I implemented this neural network architecture using the existing implmentation for the UNet in the previous section and added the necessary modules for time conditioning as outlined above. This UNet will predict the noise that is present in the image, instead of actually trying to recover the image. Mathematically, \[ \mathbb{E}_{\epsilon,z} \| \epsilon_\theta(z) - \epsilon \|^2 \] 
  
  Recall that diffusion iteratively denoises images, so we will again use the following equation: \[ x_t = \sqrt{\overline{\alpha}_t} \, x_0 + \sqrt{1- \overline{\alpha}_t} \, \epsilon \qquad \epsilon \sim \mathcal{N}(0,1) \] Specifically, we use it in the following algorithm for training the time-conditioned UNet as we want to learn the noise in an image given the current time: </p>
  <img class="nonboxed" src="images_5b/Part2/algo1_t_only.png" width="400" height="200"> 
  <p>For training, \(\sigma = 0.5\) fixed, the hidden layer has dimension 128, and use Adams optimizer with \(10^{-3}\) as the starting learning rate and an exponentially decaying learning rate. Then, to sample from the trained UNet, follow </p>
  <img class="nonboxed" src="images_5b/Part2/algo2_t_only.png" width="400" height="200"> 

  <h3>Results</h3>
  <h4>Training Loss for Time-Conditioned UNet</h4>
  <img class="nonboxed" src="images_5b/Part2/figure10.png" width="600" height="500"> 

  <h4>Sampling Outputs at different stages of training</h4>
  <table>
    <tr><td>Epoch 1</td></tr>
    <tr><td><img src="images_5b/Part2/time_cond/epoch1.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 5</td></tr>
    <tr><td><img src="images_5b/Part2/time_cond/epoch5.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 10</td></tr>
    <tr><td><img src="images_5b/Part2/time_cond/epoch10.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 15</td></tr>
    <tr><td><img src="images_5b/Part2/time_cond/epoch15.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 20</td></tr>
    <tr><td><img src="images_5b/Part2/time_cond/epoch20.png" width="600" height="400"> </td></tr>
  </table>
</div>

<div class="HomeContainer">
  <h2>2.2.4 and 2.2.5: Training and Sampling the Class-Conditioned Diffusion Model</h2>
  <h3>Approach</h3>
  <p> For this last section of the project, I implemented the same neural network architecture using the existing implmentation for the UNet in the previous section but also added the necessary modules for class conditioning. This UNet also predicts the noise that is present in the image, instead of actually trying to recover the image. Mathematically, \[ \mathbb{E}_{\epsilon,z} \| \epsilon_\theta(z) - \epsilon \|^2 \] 
  
  The training algorithm encodes the feature/class of each image as a one-hot vector and performs dropout with probability 0.1. </p>
  <img class="nonboxed" src="images_5b/Part2/algo3_c.png" width="400" height="200"> 
  <p>For training, the hidden layer has dimension 64, and use Adams optimizer with \(10^{-3}\) as the starting learning rate and an exponentially decaying learning rate. Then, to sample from the trained UNet, follow </p>
  <img class="nonboxed" src="images_5b/Part2/algo4_c.png" width="400" height="200"> 

  <h3>Results</h3>
  <h4>Training Loss for Class-Conditioned UNet</h4>
  <img class="nonboxed" src="images_5b/Part2/figure11.png" width="600" height="500"> 

  <h4>Sampling Outputs at different stages of training</h4>
  <table>
    <tr><td>Epoch 1</td></tr>
    <tr><td><img src="images_5b/Part2/class_cond/epoch1.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 5</td></tr>
    <tr><td><img src="images_5b/Part2/class_cond/epoch5.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 10</td></tr>
    <tr><td><img src="images_5b/Part2/class_cond/epoch10.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 15</td></tr>
    <tr><td><img src="images_5b/Part2/class_cond/epoch15.png" width="600" height="400"> </td></tr>
    <tr><td>Epoch 20</td></tr>
    <tr><td><img src="images_5b/Part2/class_cond/epoch20.png" width="600" height="400"> </td></tr>
  </table>
</div>

<h1>Final Thoughts</h1>
<div class="HomeContainer">
  <h2>Bells and Whistles</h2>
  <h3 style="text-align: center;"> I did not implement any B&W. </h3>
</div>

<div class="HomeContainer">
  <h2>What I enjoyed</h2>
  <p> Simply learning and having the opportunity to make our own small scale diffusion model in Part B was engaging and interesting. The first part of the project was also fun as I learned how to play around with the predicted noise to make different kinds of images. </p>
</div>

</body>
</html>